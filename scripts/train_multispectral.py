#!/usr/bin/env python3
"""
MultispectralResNetè®­ç»ƒè„šæœ¬
ç›´æ¥ä»å¤šå…‰è°±å›¾åƒè¿›è¡ŒRGBç…§åº¦ä¼°è®¡ï¼Œä¸ä½¿ç”¨ç™½ç‚¹é¢„å¤„ç†
"""

import os
import sys
import argparse
import logging
import yaml
import torch
from torch.cuda.amp import autocast, GradScaler  # æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.multispectral_dataset import create_multispectral_dataloaders
from src.models.multispectral_resnet import create_multispectral_resnet, create_pure_spectral_resnet
from src.training.loss import create_loss_function
import scipy.io as sio
import numpy as np


def setup_logging(config: dict):
    """è®¾ç½®æ—¥å¿—"""
    log_config = config.get('logging', {})
    log_level = getattr(logging, log_config.get('level', 'INFO'))
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_file = log_config.get('log_file', 'results/multispectral_resnet_logs/training.log')
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    logging.basicConfig(
        level=log_level,
        format=log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )


def load_csf_matrix(csf_path: str) -> torch.Tensor:
    """åŠ è½½ç›¸æœºå“åº”å‡½æ•°çŸ©é˜µ"""
    try:
        csf_data = sio.loadmat(csf_path)
        
        if 'CRF' in csf_data:
            csf_matrix = np.array(csf_data['CRF'], dtype=np.float32)
            if csf_matrix.shape == (3, 33):
                csf_matrix = csf_matrix[:, :31].T
            elif csf_matrix.shape == (3, 31):
                csf_matrix = csf_matrix.T
            elif csf_matrix.shape != (31, 3):
                raise ValueError(f"Unexpected CRF shape: {csf_matrix.shape}")
        else:
            # å°è¯•å…¶ä»–å¯èƒ½çš„é”®å
            possible_keys = ['csf', 'sensitivity', 'camera_sensitivity', 'response']
            csf_matrix = None
            
            for key in possible_keys:
                if key in csf_data:
                    csf_matrix = np.array(csf_data[key], dtype=np.float32)
                    break
            
            if csf_matrix is None:
                for key, value in csf_data.items():
                    if isinstance(value, np.ndarray) and not key.startswith('__'):
                        if value.shape == (31, 3):
                            csf_matrix = value.astype(np.float32)
                            break
                        elif value.shape == (3, 31):
                            csf_matrix = value.T.astype(np.float32)
                            break
                        elif value.shape == (3, 33):
                            csf_matrix = value[:, :31].T.astype(np.float32)
                            break
            
            if csf_matrix is None:
                raise ValueError(f"Could not find valid CSF matrix in {csf_path}")
            
            if csf_matrix.shape == (3, 31):
                csf_matrix = csf_matrix.T
        
        if csf_matrix.shape != (31, 3):
            raise ValueError(f"CSF matrix has invalid shape: {csf_matrix.shape}")
        
        logging.info(f"Loaded CSF matrix with shape {csf_matrix.shape}")
        return torch.from_numpy(csf_matrix)
        
    except Exception as e:
        logging.error(f"Failed to load CSF matrix from {csf_path}: {e}")
        logging.warning("Using default CSF matrix")
        return create_default_csf()


def create_default_csf() -> torch.Tensor:
    """åˆ›å»ºé»˜è®¤çš„ç›¸æœºå“åº”å‡½æ•°"""
    csf = np.zeros((31, 3), dtype=np.float32)
    csf[20:31, 0] = np.linspace(0.1, 1.0, 11)  # R
    csf[10:25, 1] = np.concatenate([np.linspace(0.1, 1.0, 8), np.linspace(1.0, 0.1, 7)])  # G
    csf[0:15, 2] = np.linspace(1.0, 0.1, 15)  # B
    return torch.from_numpy(csf)


def train_multispectral_resnet(config: dict, device: str, resume_from: str = None):
    """è®­ç»ƒå¤šå…‰è°±ResNetæ¨¡å‹"""
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_config = config['data']
    train_dir = data_config['train_dir']
    test_dir = data_config['test_dir']
    csf_path = data_config['csf_path']
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    if not os.path.isabs(train_dir):
        train_dir = project_root / train_dir
    if not os.path.isabs(test_dir):
        test_dir = project_root / test_dir
    if not os.path.isabs(csf_path):
        csf_path = project_root / csf_path
    
    logging.info(f"Loading training data from: {train_dir}")
    logging.info(f"Loading test data from: {test_dir}")
    
    # è·å–æ•°æ®åŠ è½½å™¨é…ç½®
    advanced_config = config.get('advanced', {})
    num_workers = advanced_config.get('num_workers', 4)
    persistent_workers = advanced_config.get('persistent_workers', False)
    prefetch_factor = advanced_config.get('prefetch_factor', 2)
    
    train_loader, val_loader, test_loader = create_multispectral_dataloaders(
        train_dir=str(train_dir),
        test_dir=str(test_dir),
        csf_path=str(csf_path),
        config=config,
        batch_size=config['training']['batch_size'],
        train_split_ratio=data_config['train_split_ratio'],
        num_workers=num_workers,
        random_seed=data_config['random_seed'],
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor
    )
    
    logging.info(f"Data loaded successfully:")
    logging.info(f"  Training samples: {len(train_loader.dataset)}")
    logging.info(f"  Validation samples: {len(val_loader.dataset)}")
    logging.info(f"  Test samples: {len(test_loader.dataset)}")
    
    # åŠ è½½CSFçŸ©é˜µ
    csf_matrix = load_csf_matrix(str(csf_path))
    
    # åˆ›å»ºæ¨¡å‹ - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹
    model_type = config.get('model', {}).get('type', 'multispectral_resnet')
    
    if model_type == 'pure_spectral_resnet':
        model = create_pure_spectral_resnet(config)
        logging.info(f"Created PureSpectralResNet model (End-to-End, No WP)")
    else:
        model = create_multispectral_resnet(config)
        logging.info(f"Created MultispectralResNet model")
    
    model = model.to(device)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    if hasattr(model, 'get_model_info'):
        model_info = model.get_model_info()
        logging.info(f"Model Info:")
        for key, value in model_info.items():
            logging.info(f"  {key}: {value}")
    else:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logging.info(f"Model parameters - Total: {total_params:,}, Trainable: {trainable_params:,}")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_function = create_loss_function(config, csf_matrix)
    loss_function = loss_function.to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    training_config = config['training']
    optimizer_name = training_config.get('optimizer', 'adamw').lower()
    learning_rate = training_config.get('learning_rate', 1e-4)
    weight_decay = training_config.get('weight_decay', 0.005)
    
    if optimizer_name == 'adamw':
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'sgd':
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)
    else:
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    # æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
    use_mixed_precision = config.get('advanced', {}).get('mixed_precision', False) and 'cuda' in device
    scaler = GradScaler() if use_mixed_precision else None
    
    if use_mixed_precision:
        logging.info("ğŸš€ Mixed Precision Training ENABLED (AMP)")
        logging.info("   Using automatic mixed precision for faster training")
    else:
        logging.info("Mixed Precision Training DISABLED (using FP32)")
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    scheduler_config = training_config.get('scheduler', {})
    if scheduler_config.get('type') == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=training_config.get('num_epochs', 400),
            eta_min=scheduler_config.get('eta_min', 1e-5)
        )
    elif scheduler_config.get('type') == 'step':
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=scheduler_config.get('step_size', 100),
            gamma=scheduler_config.get('gamma', 0.5)
        )
    
    # è®­ç»ƒå‚æ•°
    num_epochs = training_config.get('num_epochs', 400)
    early_stopping_patience = training_config.get('early_stopping_patience', 80)
    grad_clip_norm = training_config.get('grad_clip_norm', 1.0)
    
    # æ¨¡å‹ä¿å­˜è·¯å¾„
    model_save_path = config['validation']['model_save_path']
    os.makedirs(model_save_path, exist_ok=True)
    
    # æ–­ç”µæ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_val_loss = float('inf')
    patience_counter = 0
    
    # è‡ªåŠ¨æ£€æŸ¥æ–­ç‚¹æ¢å¤ (ä¼˜å…ˆçº§: æ‰‹åŠ¨æŒ‡å®š > latest_checkpoint > best_model)
    checkpoint_path = None
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¢å¤
    auto_resume = config.get('advanced', {}).get('auto_resume', True)
    
    if resume_from:
        # æ‰‹åŠ¨æŒ‡å®šçš„æ¢å¤è·¯å¾„ (ä¼˜å…ˆçº§æœ€é«˜)
        checkpoint_path = resume_from
        logging.info(f"Using manually specified checkpoint: {checkpoint_path}")
    elif auto_resume:
        # è‡ªåŠ¨æŸ¥æ‰¾æ£€æŸ¥ç‚¹ (æŒ‰ä¼˜å…ˆçº§)
        auto_checkpoints = [
            os.path.join(model_save_path, 'latest_checkpoint.pth'),  # æœ€æ–°è®­ç»ƒçŠ¶æ€
            os.path.join(model_save_path, 'best_model.pth'),        # æœ€ä½³æ¨¡å‹
        ]
        
        for auto_checkpoint in auto_checkpoints:
            if os.path.exists(auto_checkpoint):
                checkpoint_path = auto_checkpoint
                logging.info(f"Auto-detected checkpoint: {checkpoint_path}")
                break
    else:
        logging.info("Auto-resume disabled in config, starting fresh training")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if checkpoint_path and os.path.exists(checkpoint_path):
        try:
            logging.info(f"Loading checkpoint from {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if scheduler is not None and 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            # æ¢å¤æ··åˆç²¾åº¦scalerçŠ¶æ€
            if scaler is not None and 'scaler_state_dict' in checkpoint:
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
                logging.info("Loaded GradScaler state for mixed precision training")
            
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            patience_counter = checkpoint.get('patience_counter', 0)
            
            logging.info(f"âœ… Successfully resumed training from epoch {start_epoch}")
            logging.info(f"ğŸ“Š Best validation loss so far: {best_val_loss:.4f}")
            logging.info(f"â° Early stopping patience: {patience_counter}/{early_stopping_patience}")
        except Exception as e:
            logging.warning(f"Failed to load checkpoint {checkpoint_path}: {e}")
            logging.info("Starting training from scratch...")
    else:
        logging.info("ğŸš€ No checkpoint found, starting fresh training from epoch 0")
    
    logging.info(f"Starting training for {num_epochs} epochs (from epoch {start_epoch})...")
    
    for epoch in range(start_epoch, num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            ms_data = batch['multispectral'].to(device)  # [B, 31, H, W]
            ground_truth = batch['illumination_gt'].to(device)  # [B, 31]
            
            optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
            if scaler is not None:
                # ä½¿ç”¨æ··åˆç²¾åº¦
                with autocast():
                    predictions = model(ms_data)  # [B, 31]
                    loss = loss_function(predictions, ground_truth)
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ··åˆç²¾åº¦æ¢¯åº¦è£å‰ª
                if grad_clip_norm > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                scaler.step(optimizer)
                scaler.update()
            else:
                # æ ‡å‡†FP32è®­ç»ƒ
                predictions = model(ms_data)  # [B, 31]
                loss = loss_function(predictions, ground_truth)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if grad_clip_norm > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)
                
                optimizer.step()
            
            train_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 10 == 0:
                logging.info(f"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
        
        avg_train_loss = train_loss / num_batches
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                ms_data = batch['multispectral'].to(device)
                ground_truth = batch['illumination_gt'].to(device)
                
                # éªŒè¯æ—¶ä¹Ÿä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ
                if scaler is not None:
                    with autocast():
                        predictions = model(ms_data)
                        loss = loss_function(predictions, ground_truth)
                else:
                    predictions = model(ms_data)
                    loss = loss_function(predictions, ground_truth)
                
                val_loss += loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / val_batches
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step()
        
        # è®°å½•æ—¥å¿—
        current_lr = optimizer.param_groups[0]['lr']
        logging.info(f"Epoch {epoch:3d}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.2e}")
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'best_val_loss': best_val_loss,
                'patience_counter': patience_counter,
                'config': config
            }
            # ä¿å­˜æ··åˆç²¾åº¦scalerçŠ¶æ€
            if scaler is not None:
                checkpoint['scaler_state_dict'] = scaler.state_dict()
            
            torch.save(checkpoint, os.path.join(model_save_path, 'best_model.pth'))
            
            logging.info(f"New best model saved with val_loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ (æ¯ä¸ªepochéƒ½ä¿å­˜ï¼Œç”¨äºæ–­ç”µæ¢å¤)
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'best_val_loss': best_val_loss,
            'patience_counter': patience_counter,
            'config': config
        }
        # ä¿å­˜æ··åˆç²¾åº¦scalerçŠ¶æ€
        if scaler is not None:
            checkpoint['scaler_state_dict'] = scaler.state_dict()
        
        torch.save(checkpoint, os.path.join(model_save_path, 'latest_checkpoint.pth'))
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= early_stopping_patience:
            logging.info(f"Early stopping triggered after {epoch + 1} epochs")
            break
        
        # å®šæœŸä¿å­˜å‘½åæ£€æŸ¥ç‚¹ (å¯é€‰ï¼Œç”¨äºå¤‡ä»½)
        if epoch % 20 == 0 and epoch > 0:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'best_val_loss': best_val_loss,
                'patience_counter': patience_counter,
                'config': config
            }
            if scaler is not None:
                checkpoint['scaler_state_dict'] = scaler.state_dict()
            
            torch.save(checkpoint, os.path.join(model_save_path, f'checkpoint_epoch_{epoch}.pth'))
    
    logging.info(f"Training completed! Best validation loss: {best_val_loss:.4f}")
    
    # ä¿å­˜æœ€ç»ˆé…ç½®
    with open(os.path.join(model_save_path, 'final_config.yaml'), 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    return best_val_loss


def main():
    parser = argparse.ArgumentParser(description='Train MultispectralResNet model')
    
    parser.add_argument('--config', type=str, default='config/multispectral_resnet_config.yaml',
                       help='Path to configuration file')
    parser.add_argument('--device', type=str, default=None,
                       help='Device to use (cuda/cpu)')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"Configuration file not found: {config_path}")
        sys.exit(1)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(config)
    
    logging.info("="*60)
    logging.info("MultispectralResNet Training Started")
    logging.info("="*60)
    
    # è®¾ç½®è®¾å¤‡
    if args.device is not None:
        device = args.device
    elif config.get('device', {}).get('use_cuda', True) and torch.cuda.is_available():
        device = f"cuda:{config.get('device', {}).get('cuda_device', 0)}"
    else:
        device = 'cpu'
    
    logging.info(f"Using device: {device}")
    
    try:
        # å¼€å§‹è®­ç»ƒ
        best_val_loss = train_multispectral_resnet(config, device, args.resume)
        
        logging.info("Training completed successfully!")
        logging.info(f"Best validation loss: {best_val_loss:.4f}")
        
    except Exception as e:
        logging.error(f"Training failed with error: {e}")
        import traceback
        logging.error(traceback.format_exc())
        sys.exit(1)


if __name__ == '__main__':
    main()
